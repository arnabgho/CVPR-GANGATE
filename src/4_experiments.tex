
\section{Experiments}

\noindent To explore the effectiveness of our soft-gating residual network for image generation tasks, we test on a variety of settings:

% \section{Experiments}
% We present the details of a set of experiments performed and the corresponding results of the experiments which show the efficacy of our Gated Residual Blocks albeit being simple to implement.

% \begin{itemize}[noitemsep]
% \item {1-D unconditional modeling}
% % We show that individual blocks in a residual network self-organize into modes in a simple 1-D distribution.
% % \item {MNIST~\cite{XX} and FashionMNIST~\cite{XX} unconditional generation}
% % We show that soft-gating can help improve generations in an InfoGAN setup.
% \item {Outline$\rightarrow$Image class-conditional generation}
% \item {Edges$\rightarrow$Handbags multimodal image generation}
% \item {Disambiguating multiple disjoint tasks: Day$\rightarrow$Night and Cityscapes Label$\rightarrow$Image}
% \end{itemize}

\subsection{1D incision experiment}

% \begin{figure}[t]
%     \centering
%     %\addSubFigThird{Picture2}{ }{fig:1d_ground} 
%     \addSubFigHalf{Picture33.png}{}{fig:1d_gen} 
%     \addSubFigHalf{Picture3.png}{}{fig:1d_gen_rem}
%     \caption{{\bf 1D Mixture of Gaussians experiment} \figref{fig:1d_gen} is the training distribution, and Generated Samples from the trained network. \figref{fig:1d_gen_rem} are the Generated Samples from the trained Generator with one of the blocks removed. \ow{make prettier}}
%     \label{fig:onedexperiment}
%     \vspace{-3mm}
% \end{figure}

We first evaluate the effect of a gating network in the simple toy scenario where the data distribution is a 1D mixture of Gaussians comprising of 5 components.
We train a simple Generator and Discriminator architecture each consisting of only residual blocks where each residual block is composed of fully connected layers in place of 2D convolutions.
In this setting, we analyze the effect of removing different blocks after network training, in the vein of \cite{veit2016residual}.
We observe in Figure ~\ref{fig:onedexperiment}, when some residual blocks were disabled (and only the skip connection of that block was activated), it led to the disappearance of particular modes from the data distribution. 
Removal of groups of blocks led to the disappearance of clusters of modes from the generated distribution. 
This experiment shows that the analysis performed in the case of classification \cite{veit2016residual} also holds true in the case of generative models.


% \paragraph{Incision Experiments} similar to \cite{veit2016residual} on the generator after the network is trained. More specifically, if a layer (say $i^{th}$) had to be skipped, we disable the $f_i(x)$ of the ith residual block and now the output of the $i^{th}$ residual block is $x$ in place of the usual $x+f_i(x)$ encountered during training. Some interesting observations could be made, for example removing some blocks corresponded to the vanishing of certain modes from the generated distribution once the incision was performed on the generator network. Another surprising observation was that the same mode vanished on the incision of certain different residual blocks. This experiment validated the hypothesis of \cite{veit2016residual} that residual networks behave like an ensemble of several shallower networks and also pointed out that another network could predict based on the condition which blocks to use and to skip other non-necessary blocks in the network for that particular condition.


\subsection{Our architecture (Skinny ResNet)}
Our network design was based on the principle that deeper networks have more valid disjoint partially shared paths but to help the ease of learning for the gating mechanism we had similar number of channels for each residual block including the residual blocks used for upsampling and downsampling. All of the blocks incuding the upsampling and downsampling residual blocks were gated as the quality of generations improved with the gating applied to all layers. Details about the architecture can be found in the supplementary.

The gate prediction network was also designed using residual blocks with each residual block consisting of 1D convolution layers. 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/gen_real_vs_acc.pdf}
%     \caption{Generation realism vs accuracy}\label{fig:conditioning_amt}
%     % \vspace{-4mm}
% \end{figure}

% \paragraph{Network Architecture}
% \rz{shorten this into a few sentences for each net, so reader knows what we tested later, and point to suppmat where the tables can go}

\subsection{Outline $\rightarrow$ Image}
We introduce a new challenging dataset to test multi-class generation. 
We collected a dataset of 10 classes with 150 images each from each class, and obtained scribbles for each image using Adobe Photoshop's tool for outlining objects.
The classes collected were basketball,chicken,cookie,cupcake,moon,orange,soccer,strawberry, watermelon, pineapple. 
We further collected a test set consisting of 50 images for each class. 


Unlike previous work on scribble to image translation \cite{isola2016image2image,zhu2017unpaired,wang2017high} , these outlines contain significantly less edge information. This makes the multi-task generation problem more challenging, as the internal features must be learned by the network, and the same outline can generate substantially different images conditioned on the class. 
%The task involves multiple different objects of high realism when the input is very similar for the various classes several of them being exactly same(in the case of circular objects where the input scribble is an identical scribble for all circular classes). 

\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\linewidth]{paper_images/cond_comp2.pdf}
    \caption{{\bf Algorithm Comparison:} Various Gating Mechanisms with our Skinny Resnet architecture succeeds in generating objects from respective classes which some of the baselines fail to do. The gating is not restricted to our skinny resnet architecture and can successfully be applied to the intermediate resnet blocks in an Encoder-Decoder architecture \cite{huang2018multimodal} \label{fig:alg_comp} }
    \vspace{-4mm}
\end{figure*}


% The incision experiments as performed by \cite{veit2016residual} showed that deeper networks have more  valid paths of computation. Our incision experiments on the 1D Mixture of Gaussians setting also demonstrated the effectiveness of having deeper residual networks with lesser number of neurons in each block. We therefore design a network architecture which doesn't change the number of channels even in the case where the spatial resolution changes since the gating is applied to all the blocks including the downsampling and upsampling residual blocks and the hypothesis is that the gating mechanism would work best if all the blocks had similar number of modulations per block.


\paragraph{Evaluation}
To judge the generation quality, we perform a perceptual evaluation using Amazon Mechanical Turk, where users were shown a generated image and asked to judge how real it is.
We also tested the purity of the generations i.e. whether the class conditioning accurately guides the model to generate images from the right class by using a pretrained InceptionvV3 network finetuned on our dataset. 
The class specific generations from the test set were passed through the finetuned network and the classifications were used to judge the class specific purity of the generations.
\ow{purity is the right word? sounds weird}

As we can see from the generations \figref{fig:alg_comp}, all of the various gating mechanisms and the baselines are able to generate images of high quality and realism from some classes but most of the baselines also fail in some classes, often confusing visual features from the wrong class. 

The gating mechanism on the other hand, does not exhibit these same problems. 
Among the gating mechanisms, we found that channel-wise multiplication (on both generator and discriminator) has the best realism which is also corroborated by the Amazon Mechanical Turk Experiments and the classification accuracy of an Inception network finetuned on our dataset demonstrates the purity of the generations corresponding to the correct class using the gating mechanism.  
\paragraph{Does Naive concatenation work?} As evident from \figref{fig:alg_comp} for some classes naive concatenation helps in generating appealing images but for some classes it either fails to generate images from the right class or generates unrealistic images from that class.
% No, point to Figure 9 (qualitative grid) \& Figure 10 (the plot)

% \paragraph{Does gating produce accurate and realistic conditional generations?} The gating mechanisms succeeds in generating realistic images from each class while also preserving the class conditioning appropriately. As evident from the comparisons of generations in \figref{fig:alg_comp} and the metrics reported in \figref{fig:conditioning_amt} the channel wise multiplication produces the most visually appealing results amongst the various gating mechanisms.
% Yes, point to Figure 9 results. Also analyze which variation is the best

\paragraph{Does gating work across architectures?} 
We evaluate the following architectures on this task: 
We first evaluate our \todo{SkinnyResNet} with a more traditional encoder-decoder architecture. 
We use the architecture proposed in MUNIT~\cite{huang2018multimodal} which consists of an encoder followed by a set of residual blocks with a decoder at the end.
In this case, we apply the gating to this architecture only on the bottleneck residual blocks.
As evident from \figref{fig:alg_comp} although the network learns to disentangle the various classes, the generation quality reduces from the skinny resnet architecture where all the layers are gated.

% Yes, works for both skinny resnet \& enc-dec.

\paragraph{Do the generations generalize to unusual outlines?} The training images consisted of the outlines corresponding to the accurate geometry for each class but the technique generalized to shapes it had never encountered during training. As evident from \figref{fig:teaser} in cases of strawberry, pineapple ,cupcake and fried chicken the network is able to generate realistic textures even when the conditioning input is in the form of a circle.
% Yes, point to circular pineapple in Figure 1

\paragraph{How are the gating coefficients distributed?}
An interesting observation of the gating coefficients in the case of the block wise gating mechanism, as demonstrated in \figref{fig:gated_block_act} for different classes different sets of blocks were activated and some blocks were totally deactivated for some classes. The switching on and off of various blocks without even adding a sparsity constraint was better defined in the case of the discriminator.


% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=.94\linewidth]{paper_images/cond_comp2.pdf}
%     \caption{{\bf Algorithm Comparison} \label{fig:alg_comp} }
%     \vspace{-4mm}
% \end{figure*}



% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{paper_images/cond_comp.pdf}
%     \caption{Algorithm Comparison}\label{fig:conditioning_amt}
% \end{figure}

% In the widely popular image conditioned generative models introduced in Pix2pix by \cite{isola2016image2image} although the results are brilliant, it had the inherent problem of only being applicable to a particular task such as different networks had to be trained for edges to shoes and edges to handbags, although StarGAN \cite{choi2017stargan} mitigated some of the problems but it was only applicable for relatively minute transformations such as changing the characteristics of the face.

% To analyze properly the task of multi-class generations in the image conditioned setting we introduce a new task of generating class conditioned realistic images from very rough scribbles. We start off with 3 classes, namely: pizza, strawberry and oranges. A simple pix2pix network fails to identify the different classes and starts injecting weird textures such as pizza on orange or strawberry on pizza as depicted in the results from pix2pix on this task in \figref{fig:scribble_pix2pix}. 

% In the conditional setting for the generator, the main block only receives the input scribble and the gate selection block receives the class condition and predicts the $aplha^i$s for the gated residual blocks. The discriminator on the other hand is composed of a main network consisting of gated residual blocks which is oblivious to the class conditioning and a gate selection network which predicts the $aplha^i$s for the main network of the discriminator. The main network also receives the input scribble alongside the generated/real image to predict how real/fake an image is based on the alpha weightings of its gated residual blocks. The network is able to disentangle the class conditioning although none of the main networks of the generator/ discriminator are aware of the class conditioning, the class information only being input to the gate selection network which has to modulate the weights of the respective networks' gated residual blocks. The results of the model are depicted in \figref{fig:scribble_grb} whereby we can clearly see that the network has been able to disentangle the class conditioning and generate textures appropriate for the right class.


% \subsection{Comparison to other forms of conditioning in Resblock :}
% Conditional Batch Normalization, FiLM and Adaptive Instance Normalization are the techniques which are the most closely related to our methodology. All of these have the benefit of being applicable even without the presence of residual blocks in the architecture. So an exhaustive comparison with all of these methods along with our form of conditioning on the residual blocks is a valid set of experimentation and has to be done to make the paper complete.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{conditioning_amt.png}
%     \caption{AMT studies of the various gating techniques on the scribble dataset}\label{fig:conditioning_amt}
%     \vspace{-4mm}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{infogan.jpg}
    \caption{Naive Conditioning of infogan fails(left) while gating (right) succeeds in producing diverse generations}\label{fig:infogan_gate}
    \vspace{-4mm}
\end{figure}

% \subsection{Infogan Variations(pix2pix):}
\subsection{Edges $\rightarrow$ Handbags (Multimodal Generations)}
Inter-class variation, also called diversity, is a significant challenge in GAN image generation applications. 
In the pix2pix setting as shown by previous works \cite{ghosh2017multi} and \cite{zhu2017toward} even the InfoGAN (cLR in \cite{zhu2017toward}) setup wasn't able to produce meaningful variations in the generations, or needed multiple generators or cyclical losses to produce meaningful variations in the generated images. 
With our gating mechanism, we show that we can create meaningful variations in the generated samples conditioned on a single sketch as evident from the generations \figref{fig:infogan_gate} and from the LPIPS metric \cite{zhang2018unreasonable} in  Table. \ref{table:infogan_lpips} which measures the diversity among the generations in the feature space of a standard Imagenet classifier.
\ow{needs more detail}

\begin{table}[ht]
\caption{Evaluation on Edges to Handbags diverse generations. Diversity using LPIPS metric \cite{zhang2018unreasonable}} % title of Table
\small
\centering % used for centering table
% \begin{tabular}{|c|c|c|c|} % centered columns (4 columns)
\begin{tabular}{p{3cm}p{3cm}} % centered columns (4 columns)
% \hline
\toprule
\textbf{Model} & \textbf{LPIPS Distance} \\%heading
\midrule
BicycleGAN \cite{zhu2017toward} & $0.1374 \pm 0.0005$  \\ % inserting body of the table
\midrule
Ours(Gated) & $0.0964 \pm 0.0003$  \\
\midrule
Concat(Input) &  $0.0432 \pm 0.0002$ \\
\midrule
Concat(All Layers) & $0.0159 \pm 0.0004$ \\
\midrule
Random Real Images & $0.3665 \pm 0.0053$ \\
\bottomrule %inserts single line
\end{tabular}
\label{table:infogan_lpips} % is used to refer this table in the text
\end{table}





\begin{figure}[t]%[ht!]
\centering
\begin{tabular}{*{5}{c@{\hspace{3px}}}}
    \includegraphics[width=.18\linewidth]{channel_gated/cityscapes_95_real_A} &
    \includegraphics[width=.18\linewidth]{channel_gated/cityscapes_95_fake_B} &
    \includegraphics[width=.18\linewidth]{munit_baseline_all/cityscapes_95_fake_B} &
    \includegraphics[width=.18\linewidth]{our_baseline_all/cityscapes_95_fake_B} & 
    \includegraphics[width=.18\linewidth]{acgan_baseline_all/cityscapes_95_fake_B} \\
    
    \includegraphics[width=.18\linewidth]{channel_gated/night2day_58_5018_to_5000_real_A} &
    \includegraphics[width=.18\linewidth]{channel_gated/night2day_58_5018_to_5000_fake_B} &
    \includegraphics[width=.18\linewidth]{munit_baseline_all/night2day_58_5018_to_5000_fake_B} &
    \includegraphics[width=.18\linewidth]{our_baseline_all/night2day_58_5018_to_5000_fake_B} &
    \includegraphics[width=.18\linewidth]{acgan_baseline_all/night2day_58_5018_to_5000_fake_B} \\
    \begin{subfigure}[t]{.18\linewidth}\caption{\small Input}\label{fig:daynightinput}\end{subfigure} &
    \begin{subfigure}[t]{.18\linewidth}\caption{\small Ours}\label{fig:daynightinput}\end{subfigure} &
    \begin{subfigure}[t]{.18\linewidth}\caption{\small Enc-Dec Concat (All)}\label{fig:daynightinput}\end{subfigure} &
    \begin{subfigure}[t]{.18\linewidth}\caption{\small Concat (All)}\label{fig:daynightinput}\end{subfigure} &
    \begin{subfigure}[t]{.18\linewidth}\caption{\small Concat(All)+Aux-Class}\label{fig:daynightinput}\end{subfigure}\\
\end{tabular}
    % \addSubFigEighth{channel_gated/cityscapes_95_real_A}{input}{fig:city_input} 
    % \addSubFigEighth{channel_gated/cityscapes_95_fake_B}{Ours}{fig:city_ours} 
    % \addSubFigEighth{munit_baseline_all/cityscapes_95_fake_B}{MUNIT Conditioning all layers}{fig:city_munit}
    % \addSubFigEighth{our_baseline_all/cityscapes_95_fake_B}{Our Architecture Conditioning all layers}{fig:bag_3}
    % \addSubFigEighth{acgan_baseline_all/cityscapes_95_fake_B}{ACAN (our Architecture Conditioning all layers) }{fig:bag_3}
    % \caption{Various Methods for Cityscapes for Multi-Task Experiment}
    % \label{fig:multi-task_cityscapes}
    % \vspace{-3mm}
    % \addSubFigEighth{channel_gated/night2day_58_5018_to_5000_real_A}{input}{fig:city_input} 
    % \addSubFigEighth{channel_gated/night2day_58_5018_to_5000_fake_B}{Ours}{fig:city_ours} 
    % \addSubFigEighth{munit_baseline_all/night2day_58_5018_to_5000_fake_B}{MUNIT Conditioning all layers}{fig:city_munit}
    % \addSubFigEighth{our_baseline_all/night2day_58_5018_to_5000_fake_B}{Our Architecture Conditioning all layers}{fig:bag_3}
    % \addSubFigEighth{acgan_baseline_all/night2day_58_5018_to_5000_fake_B}{ACAN (our Architecture Conditioning all layers) }{fig:bag_3}
    \caption{Results on the multi-task generation problem with Segmentation2Cityscapes and Day2Night. \ow{seems like this is missing 1. two generators, and 2. one generator. instead it seems more like an ablation study on our method, which is not the point. would be more impressive to show it matches 2 generators and 1 generator totally fails.}}
    \label{fig:multi-task_day2night}
    \vspace{-3mm}
\end{figure}


\begin{table}[ht]
\caption{Evaluation on Cityscapes for Multi-Task scenario. A pre-trained segmentation network~\cite{} performs a semantic segmentation on generated images, and the result is compared to the ground truth.} % title of Table
\small
\centering % used for centering table
% \begin{tabular}{|c|c|c|c|} % centered columns (4 columns)
\begin{tabular}{p{3cm}p{1cm}p{1cm}p{1cm}} % centered columns (4 columns)
% \hline
\toprule
\textbf{Model} & \textbf{Per-pixel acc.} &  \textbf{Per-class acc.} & \textbf{Class IOU} \\%heading
\midrule
Pix2pix \cite{isola2016image2image} & 66 \%  & 0.23 & 0.17 \\ % inserting body of the table
\midrule
Ours(Channel-Gate) & 68.4 \%  & 0.23 & 0.18 \\
\midrule
Concat(All),Aux-class & 67.5 \% & 0.2 & 0.16 \\
\midrule
Concat(All) & 60.2 \% & 0.18 & 0.16 \\
\midrule
Enc-Dec, Concat(All) & 60 \% & 0.16 & 0.15 \\
\bottomrule %inserts single line
\end{tabular}
\label{table:1d_G} % is used to refer this table in the text
\end{table}

\subsection{Multi-Task Generations: }
We also evaluate two previously shown image to image translation tasks, using the same network. 
%Since our network is robust enough to be able to generate images conditioned on class and modulation of which blocks to use, it can further be used to generate images which are different in tasks such as the same network
We try to solve both segmentation map $\rightarrow$ street scene, and day $\rightarrow$ night image translation tasks using the same generator. 
As we can see in \figref{fig:multi-task_day2night}, our model (Channel-Gate) is able to perform both tasks while the other techniques generate mixed results. 

In case of Encoder Decoder architecture \cite{huang2018multimodal} conditioned on all layers the discriminator became too powerful after a while and the training progressed with only the L1 loss which caused the blurry generations in the case of segmentation maps to real image synthesis as evident from the figure. The other baselines based on our skinny resnet architecture failed to generate realistic images in the case of the task of day2night.



% \section{Planned Set of Experiments:}
% \subsection{Unconditional Generations :}
% Since the model is not restricted to be applicable only in the image to image setting and is more general than that, if we get the unconditional generations working at least on the places dataset and the faces(it also has got some class labels that can be used for conditioned generation). It will be a good generalization. A bit of engineering might be required to get the architecture working on the unconditional case since the structure of the generator and discriminator would be different from the current generator which takes an image as an input and outputs an image of the same resolution. The discriminator in the case of the current image2image experiments employ a patch based discriminator which has to be modified to work in the unconditional generation setting.




% \subsection{Instance Based Generations: }
% As demonstrated by the early experiments I performed with pix2pixhd that it overfits the dataset and simplification of the same instance map led to garbled generations showed that it was unstable to perturbations. Instance based generations could be possible with our model since we already know which pixels are for which class and we can generate instances of each class separately and then stitch together the various class generations into a final image.


% \section{Directions about novelty of approach:} 

% \subsection{Learning to Learn}
% Learning to learn is becoming an increasingly relevant paradigm for deep learning models as the power of the networks increase and we want less hyper parameter tuning. Our model can be perceived as a system whereby the hypernetwork responsible for predicting the $\alpha_i$ of each block is learning by analyzing the function learnt by each block and thereby distributing the blocks between the various different classes for the generator and the discriminator. We will have to look deeply into the literature to identify the connections with the learning to learn paradigm.

% \subsection{Ensembles of several shallower nets (implicit MAD-GAN)}
% The initial motivation of Eli and Oliver was to extend MAD-GAN with the intuitions gained by Andreas Veit's paper on Residual Blocks acting as ensembles of several shallower networks. The structure of the paper at the moment follows that intuition and the experiments on the 1D Mixture of Gaussians demonstrates that residual blocks indeed behave as ensembles of several shallower networks even in the case of generative models.

% \subsection{Effective way of fusing high dimensional and low dimensional information}
% The techniques proposed in this paper provides a way of effectively fusing high dimensional information in the form of images and low dimensional conditioning in terms of class conditioning or sampled conditioning as in the form of InfoGAN. Further the multi-task application demonstrates the efficacy of the fusing of task information alongside the high dimensional information about images.

