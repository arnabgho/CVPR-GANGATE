
\section{Experiments}
We present the details of a set of experiments performed and the corresponding results of the experiments which show the efficacy of our Gated Residual Blocks albeit being simple to implement.

\subsection{Non Parametric Density Estimation:}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Picture2}
    \caption{Ground Truth Distribution }\label{fig:1d_ground}
    \vspace{-4mm}
\end{figure}
In order to understand the behavior of the various residual blocks, we first perform a very simple synthetic experiment, much easier than generating high-dimensional complex images. We consider a distribution of 1D Gaussian Mixture \cite{bishop2007pattern} having five mixture components with modes at 10, 20, 60, 80 and 110, and standard deviations of 3, 3, 2, 2 and 1, respectively. While the first two modes overlap significantly, the fifth mode stands isolated as shown in \figref{fig:1d_ground}. We train our resnet block based GAN model using 1 million samples from this distribution and generate 1 million samples from the trained model. In order to compare the learned distribution with the ground truth distributions, we first estimate them using bins over the data points and create the histograms. These histograms are carefully created using different bin sizes and the best bin (found to be 0.1) is chosen. The generated distribution from the trained model corresponds very closely to the ground truth distribution \figref{fig:1d_gen}. The interesting aspect was that although the network was deeper (16 layers of residual blocks) than required for similar experiments in MAD-GAN \cite{ghosh2017multi}, Mode Regularized GAN \cite{che2016mode} and Unrolled GAN \cite{metz2017unrolledGAN}, there were only 4 neurons in each residual block with 16 layers of 4 neurons each in the generator and discriminator compared to fully connected versions in which there consisted of connections between 256 neurons in the preceding layer to 256 neurons in the current layer. Thus although the number of parameters were less, the network learnt the distribution quite accurately.

The next set of experiments was the incision experiments similar to \cite{veit2016residual} on the generator after the network is trained. More specifically, if a layer (say $i^{th}$) had to be skipped, we disable the $f_i(x)$ of the ith residual block and now the output of the $i^{th}$ residual block is $x$ in place of the usual $x+f_i(x)$ encountered during training. Some interesting observations could be made, for example removing some blocks corresponded to the vanishing of certain modes from the generated distribution once the incision was performed on the generator network. Another surprising observation was that the same mode vanished on the incision of certain different residual blocks. This experiment validated the hypothesis of \cite{veit2016residual} that residual networks behave like an ensemble of several shallower networks and also pointed out that another network could predict based on the condition which blocks to use and to skip other non-necessary blocks in the network for that particular condition.




\subsection{InfoGAN based Model}
The intuitions gained from the experiments performed for the non-parametric density estimation led to the Gated Residual Block on the Generator of a GAN. InfoGAN \cite{chen2016infogan} presented a perfect setting to apply the Gated Residual Block in the case of the generator. The first set of experiments were on MNIST and Fashion-MNIST with sets of 10 discrete variables (since there were 10 classes in both of the datasets) and 2 continuous variables whose information was tried to be maximized using the Q network. As can be seen in \figref{fig:infogan_unconditional} the network was able to generate realistic images from the different classes and produce meaningful interpolations by varying the continuous variables as shown in the figure. The exact experimental setting was that the hypernetwork responsible for the predictions of the $alpha^i$s which we term as the gate selection network gets the salient variables and has no information about the random noise sampled for the main network(consisting of gated residual blocks), based on the salient variables received the gate selection network predicts the  $alpha^i$s for all the blocks of the network. The main network is oblivious of the conditioning received in the salient variables and has to generate images coherent with the conditioning since the Q-Network tries to reconstruct back the conditioning received in the form of salient variables. 

\begin{figure}%[ht!]
    \centering
    \addSubFigHalf{Picture34}{MNIST}{fig:infogan_mnist} 
    \addSubFigHalf{Picture35}{Fashion-MNIST}{fig:infogan_fashion_mnist} 
    \caption{Gated Residual Block-InfoGAN on MNIST and Fashion MNIST}
    \label{fig:infogan_unconditional}
    \vspace{-3mm}
\end{figure}

A well known problem in image conditional GAN settings is the tendency to produce realistic outputs but not being able to produce meaningful variations in the generated output. It was especially prevalent in the original pix2pix setting \cite{isola2016image2image} and further research enabled variations such as \cite{ghosh2017multi} and \cite{zhu2017toward}. Although as shown in \cite{ghosh2017multi} the naive infoGAN setting couldn't produce meaningful variations in the pix2pix setting, settling rather for very minute variations. Our Gated Residual Blocks could mitigate some of the problems and did produce variations in the challenging task of edges-to-bag generation as has been demonstrated in \figref{fig:infogan_bags}. By varying the continuous variable smooth interpolations could be obtained between colors and textures which was earlier not possible with the infogan based pix2pix setting. Although the variations along the discrete axis was quite minimal and we are still investigating the cause for the same. The experimental setting in this scenario was that the salient variables were only given to the gate selection network and had to predict the $alpha^i$s for the main network which consisted of gated residual blocks and was oblivious of the conditioning provided in the form of salient variables. The Discriminator network was the standard image conditional discriminator network while the Q network in this scenario was modified to take the generated image as well as the edge map to behave as a image conditional Q Network which tries to reconstruct back the salient variables given to the gate selection network. 

\newcommand{\addSubFigEighth}[3]{\begin{subfigure}[t]{.18\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}
\begin{figure*}%[ht!]
    \centering
    \addSubFigEighth{Picture8}{Sketch}{fig:bag_sketch} 
    \addSubFigEighth{Picture9}{Varition 1}{fig:bag_1} 
    \addSubFigEighth{Picture10}{Varition 2}{fig:bag_2}
    \addSubFigEighth{Picture11}{Varition 3}{fig:bag_3}
    \addSubFigEighth{Picture12}{Varition 4}{fig:bag_4}
    \caption{The variations produced in the sketch to realistic bag task in the infogan with Gated Residual Block setting for the generator }
    \label{fig:infogan_bags}
    \vspace{-3mm}
\end{figure*}

\subsection{Unconditional Generations}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Picture5}
    \caption{Results on the Gated Residual Block on MNIST dataset}\label{fig:grb_mnist}
    \vspace{-4mm}
\end{figure}

The next set of experiments were performed to judge the robustness of the Gate Selection Blocks and the Gated Residual Blocks in the case the labels were available. That meant using the gate selection network and the gated residual blocks on the discriminator. The implications were even more interesting, the gate selection network would have to distribute the blocks between the classes to get the class specific gradients back for training the class conditioned generator. In the unconditional setting for the generator, the main block only receives random noise and the gate selection block receives the class condition and predicts the $aplha^i$s for the gated residual blocks. The discriminator on the other hand is composed of a main network consisting of gated residual blocks which is oblivious to the class conditioning and a gate selection network which predicts the $aplha^i$s for the main network of the discriminator. The main network predicts how real/fake an image is based on the alpha weightings of its gated residual blocks. The network is able to disentangle the class conditioning although none of the main networks of the generator/ discriminator are aware of the class conditioning, the class information only being input to the gate selection network which has to modulate the weights of the respective networks' gated residual blocks. The generated samples on the MNIST dataset are shown in \figref{fig:grb_mnist} and the alpha gatings for the different classes and the different blocks in \figref{fig:mnist_act}


\begin{figure}%[ht!]
    \centering
    \addSubFigHalf{Picture6}{Activations of the gated residual blocks in the generator}{fig:gen_act} 
    \addSubFigHalf{Picture7}{Activations of the gated residual blocks in the discriminator}{fig:dis_act} 
    \caption{Activation of the various blocks in the Generator and Discriminator}
    \label{fig:mnist_act}
    \vspace{-3mm}
\end{figure}




\subsection{Image Conditional Generations}

\paragraph{Scribble Dataset:}
To test the capability of our techniques on the varied kinds of generation tasks, we collected a dataset of 10 classes with 150 images each from each class. We obtained the scribbles using Adobe Photoshop's tool for outlining objects. We further had a test set consisting of 50 images for each class. The classes collected were basketball,chicken,cookie,cupcake,moon,orange,soccer,strawberry, watermelon, pineapple. 

\newcommand{\addSubFigTenth}[3]{\begin{subfigure}[t]{.16\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}

\begin{figure*}%[ht!]
    \centering
    \addSubFigTenth{acgan_baseline_all/basketball_11_real_A}{}{fig:basketball_scribble} 
    \addSubFigTenth{acgan_baseline_all/chicken_2_real_A}{}{fig:chicken_scribble} 
    \addSubFigTenth{acgan_baseline_all/cookie_13_real_A}{}{fig:cookie_scribble}
    \addSubFigTenth{acgan_baseline_all/cupcake_27_real_A}{}{fig:cupcake_scribble}
    \addSubFigTenth{acgan_baseline_all/moon_15_real_A}{}{fig:moon_scribble}
    \addSubFigTenth{acgan_baseline_all/basketball_11_fake_B}{}{fig:basketball_img} 
    \addSubFigTenth{acgan_baseline_all/chicken_2_fake_B}{}{fig:chicken_img} 
    \addSubFigTenth{acgan_baseline_all/cookie_13_fake_B}{}{fig:cookie_img}
    \addSubFigTenth{acgan_baseline_all/cupcake_27_fake_B}{}{fig:cupcake_img}
    \addSubFigTenth{acgan_baseline_all/moon_15_fake_B}{}{fig:moon_img}
    \addSubFigTenth{acgan_baseline_all/orange_17_real_A}{}{fig:orange_scribble} 
    \addSubFigTenth{acgan_baseline_all/pineapple_2_real_A}{}{fig:pineapple_scribble} 
    \addSubFigTenth{acgan_baseline_all/soccer_18_real_A}{}{fig:soccer_scribble}
    \addSubFigTenth{acgan_baseline_all/strawberry_1_real_A}{}{fig:strawberry_scribble}
    \addSubFigTenth{acgan_baseline_all/watermelon_17_real_A}{}{fig:watermelon_scribble}
    \addSubFigTenth{acgan_baseline_all/orange_17_fake_B}{}{fig:orange_img} 
    \addSubFigTenth{acgan_baseline_all/pineapple_2_fake_B}{}{fig:pineapple_img} 
    \addSubFigTenth{acgan_baseline_all/soccer_18_fake_B}{}{fig:soccer_img}
    \addSubFigTenth{acgan_baseline_all/strawberry_1_fake_B}{}{fig:strawberry_img}
    \addSubFigTenth{acgan_baseline_all/watermelon_17_fake_B}{}{fig:watermelon_img}
    \caption{ACGAN baseline with Input Provided to all layers of Generator}
    \label{fig:scribble_pix2pix}
    \vspace{-3mm}
\end{figure*}


\begin{figure*}%[ht!]
    \centering
    \addSubFigTenth{channel_gated/basketball_11_real_A}{}{fig:basketball_scribble} 
    \addSubFigTenth{channel_gated/chicken_2_real_A}{}{fig:chicken_scribble} 
    \addSubFigTenth{channel_gated/cookie_13_real_A}{}{fig:cookie_scribble}
    \addSubFigTenth{channel_gated/cupcake_27_real_A}{}{fig:cupcake_scribble}
    \addSubFigTenth{channel_gated/moon_15_real_A}{}{fig:moon_scribble}
    \addSubFigTenth{channel_gated/basketball_11_fake_B}{}{fig:basketball_img} 
    \addSubFigTenth{channel_gated/chicken_2_fake_B}{}{fig:chicken_img} 
    \addSubFigTenth{channel_gated/cookie_13_fake_B}{}{fig:cookie_img}
    \addSubFigTenth{channel_gated/cupcake_27_fake_B}{}{fig:cupcake_img}
    \addSubFigTenth{channel_gated/moon_15_fake_B}{}{fig:moon_img}
    \addSubFigTenth{channel_gated/orange_17_real_A}{}{fig:orange_scribble} 
    \addSubFigTenth{channel_gated/pineapple_2_real_A}{}{fig:pineapple_scribble} 
    \addSubFigTenth{channel_gated/soccer_18_real_A}{}{fig:soccer_scribble}
    \addSubFigTenth{channel_gated/strawberry_1_real_A}{}{fig:strawberry_scribble}
    \addSubFigTenth{channel_gated/watermelon_17_real_A}{}{fig:watermelon_scribble}
    \addSubFigTenth{channel_gated/orange_17_fake_B}{}{fig:orange_img} 
    \addSubFigTenth{channel_gated/pineapple_2_fake_B}{}{fig:pineapple_img} 
    \addSubFigTenth{channel_gated/soccer_18_fake_B}{}{fig:soccer_img}
    \addSubFigTenth{channel_gated/strawberry_1_fake_B}{}{fig:strawberry_img}
    \addSubFigTenth{channel_gated/watermelon_17_fake_B}{}{fig:watermelon_img}
    \caption{Gated (Channel/Alpha) Results}
    \label{fig:scribble_pix2pix}
    \vspace{-3mm}
\end{figure*}


\newcommand{\addSubFigSixth}[3]{\begin{subfigure}[t]{.30\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}
% \begin{figure*}%[ht!]
%     \centering
%     \addSubFigSixth{Picture27}{Pizza On Orange}{fig:bag_sketch} 
%     \addSubFigSixth{Picture28}{Pizza on Pizza}{fig:bag_1} 
%     \addSubFigSixth{Picture29}{Pizza on Orange}{fig:bag_2}
%     \addSubFigSixth{Picture30}{Pizza on Strawberry}{fig:bag_3}
%     \addSubFigSixth{Picture31}{Strawberry on Orange}{fig:bag_4}
%     \addSubFigSixth{Picture32}{Orange on Orange}{fig:bag_4}
%     \caption{Pix2pix fails when there are multi-class mappings from similar scribbles to the varying classes}
%     \label{fig:scribble_pix2pix}
%     \vspace{-3mm}
% \end{figure*}

% \begin{figure*}%[ht!]
%     \centering
%     \addSubFigSixth{Picture15}{Pizza 1}{fig:pizza_1} 
%     \addSubFigSixth{Picture16}{Pizza 2}{fig:pizza_2} 
%     \addSubFigSixth{Picture17}{Pizza 3}{fig:pizza_3}
%     \addSubFigSixth{Picture19}{Strawberry 1}{fig:strawberry_1}
%     \addSubFigSixth{Picture20}{Strawberry 2}{fig:strawberry_2}
%     \addSubFigSixth{Picture21}{Strawberry 3}{fig:strawberry 3}
%     \addSubFigSixth{Picture23}{Orange 1}{fig:orange_1}
%     \addSubFigSixth{Picture24}{Orange 2}{fig:orange_2}
%     \addSubFigSixth{Picture25}{Orange 3}{fig:orange_3}
%     \caption{Pix2pix fails when there are multi-class mappings from similar scribbles to the varying classes}
%     \label{fig:scribble_grb}
%     \vspace{-3mm}
% \end{figure*}

In the widely popular image conditioned generative models introduced in Pix2pix by \cite{isola2016image2image} although the results are brilliant, it had the inherent problem of only being applicable to a particular task such as different networks had to be trained for edges to shoes and edges to handbags, although StarGAN \cite{choi2017stargan} mitigated some of the problems but it was only applicable for relatively minute transformations such as changing the characteristics of the face.

To analyze properly the task of multi-class generations in the image conditioned setting we introduce a new task of generating class conditioned realistic images from very rough scribbles. We start off with 3 classes, namely: pizza, strawberry and oranges. A simple pix2pix network fails to identify the different classes and starts injecting weird textures such as pizza on orange or strawberry on pizza as depicted in the results from pix2pix on this task in \figref{fig:scribble_pix2pix}. 

In the conditional setting for the generator, the main block only receives the input scribble and the gate selection block receives the class condition and predicts the $aplha^i$s for the gated residual blocks. The discriminator on the other hand is composed of a main network consisting of gated residual blocks which is oblivious to the class conditioning and a gate selection network which predicts the $aplha^i$s for the main network of the discriminator. The main network also receives the input scribble alongside the generated/real image to predict how real/fake an image is based on the alpha weightings of its gated residual blocks. The network is able to disentangle the class conditioning although none of the main networks of the generator/ discriminator are aware of the class conditioning, the class information only being input to the gate selection network which has to modulate the weights of the respective networks' gated residual blocks. The results of the model are depicted in \figref{fig:scribble_grb} whereby we can clearly see that the network has been able to disentangle the class conditioning and generate textures appropriate for the right class.

\begin{table}[ht]
\caption{ConvResblock} % title of Table
\centering % used for centering table
\begin{tabular}{c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
F(x)\\%heading
\hline
Conv2d(nfilters,kernel=3,stride=1,padding=1) \\
InstanceNorm2d(nfilters)\\ % inserting body of the table
ReLU() \\
Conv2d(nfilters,kernel=3,stride=1,padding=1) \\
InstanceNorm2d(nfilters)\\ % inserting body of the table
ReLU() \\
\hline %inserts single line
\end{tabular}
\label{table:convresblock} % is used to refer this table in the text
\end{table}


\begin{table}[ht]
\caption{UpConvResblock} % title of Table
\centering % used for centering table
\begin{tabular}{c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
F(x)\\%heading
\hline
Upsample (Nearest Neighbor) \\
ReflectionPad2d(1) \\
Conv2d(nfilters,kernel=3,stride=1,padding=0) \\
InstanceNorm2d(nfilters)\\ % inserting body of the table
ReLU() \\
Conv2d(nfilters,kernel=3,stride=1,padding=1) \\
InstanceNorm2d(nfilters)\\ % inserting body of the table
ReLU() \\
\hline %inserts single line
Shortcut\\
\hline 
Upsample (Nearest Neighbor) \\
ReflectionPad2d(1)\\
Conv2d(nfilters,kernel=3,stride=1,padding=0) \\
\hline
\end{tabular}
\label{table:upconvresblock} % is used to refer this table in the text
\end{table}

\begin{table}[ht]
\caption{DownConvResblock} % title of Table
\centering % used for centering table
\begin{tabular}{c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
F(x)\\%heading
\hline
Avgpool 2d \\
ReflectionPad2d(1) \\
Conv2d(nfilters,kernel=3,stride=1,padding=0) \\
InstanceNorm2d(nfilters)\\ % inserting body of the table
ReLU() \\
Conv2d(nfilters,kernel=3,stride=1,padding=1) \\
InstanceNorm2d(nfilters)\\ % inserting body of the table
ReLU() \\
\hline %inserts single line
Shortcut\\
\hline 
Avgpool 2d \\
ReflectionPad2d(1)\\
Conv2d(nfilters,kernel=3,stride=1,padding=0) \\
\hline
\end{tabular}
\label{table:downconvresblock} % is used to refer this table in the text
\end{table}


\begin{table}[ht]
\caption{Generator} % title of Table
\centering % used for centering table
\begin{tabular}{c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
layer & num layers\\%heading
\hline % inserts single horizontal line
Linear(10,4) & 1\\ % inserting body of the table
ResBlock & 16 \\
Linear(4,1) & 1 \\
\hline %inserts single line
\end{tabular}
\label{table:1d_G} % is used to refer this table in the text
\end{table}


\subsection{Comparison to other forms of conditioning in Resblock :}
Conditional Batch Normalization, FiLM and Adaptive Instance Normalization are the techniques which are the most closely related to our methodology. All of these have the benefit of being applicable even without the presence of residual blocks in the architecture. So an exhaustive comparison with all of these methods along with our form of conditioning on the residual blocks is a valid set of experimentation and has to be done to make the paper complete.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{conditioning_amt.png}
    \caption{AMT studies of the various gating techniques on the scribble dataset}\label{fig:conditioning_amt}
    \vspace{-4mm}
\end{figure}

The generations on the test set was further corroborated by 

\subsection{Infogan Variations(pix2pix):}
In the pix2pix setting as shown by previous works \cite{ghosh2017multi} and \cite{zhu2017toward} even the InfoGAN setup wasn't able to produce meaningful variations in the generations and needed different generators or 2 cycles respectively to produce meaningful variations in the generated images. With our best gating mechanism (Channel wise multiplication), it leads to meaningful variations albeit might be lesser than the other 2 methods \cite{ghosh2017multi} ,\cite{zhu2017toward}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{infogan.jpg}
    \caption{Naive Conditioning of infogan fails while gating succeeds}\label{fig:infogan_gate}
    \vspace{-4mm}
\end{figure}

\subsection{Multi-Task Generations: }
Since our network is robust enough to be able to generate images conditioned on class and modulation of which blocks to use, it can further be used to generate images which are different in tasks such as the same network could do both day2night and instance maps to realistic generations of street scenes, efficacy in this dataset would show the efficiency of our method as well as Richard pointed out its a well known task which people accept in which evaluation is also slightly easier.


\begin{figure*}%[ht!]
    \centering
    \addSubFigEighth{channel_gated/cityscapes_95_real_A}{}{fig:bag_sketch} 
    \addSubFigEighth{channel_gated/cityscapes_95_fake_B}{}{fig:bag_1} 
    \addSubFigEighth{channel_gated/night2day_58_5018_to_5000_real_A}{}{fig:bag_2}
    \addSubFigEighth{channel_gated/night2day_58_5018_to_5000_fake_B}{}{fig:bag_3}
    \caption{Gated(Channel/Alpha) Multi-Task}
    \label{fig:channel_gated_multi-task}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}%[ht!]
    \centering
    \addSubFigEighth{unet_all/cityscapes_95_real_A}{}{fig:bag_sketch} 
    \addSubFigEighth{unet_all/cityscapes_95_fake_B}{}{fig:bag_1} 
    \addSubFigEighth{unet_all/night2day_58_5018_to_5000_real_A}{}{fig:bag_2}
    \addSubFigEighth{unet_all/night2day_58_5018_to_5000_fake_B}{}{fig:bag_3}
    \caption{U-Net (conditioning all layers) Multi-Task}
    \label{fig:channel_gated_multi-task}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}%[ht!]
    \centering
    \addSubFigEighth{our_baseline_all/cityscapes_95_real_A}{}{fig:bag_sketch} 
    \addSubFigEighth{our_baseline_all/cityscapes_95_fake_B}{}{fig:bag_1} 
    \addSubFigEighth{our_baseline_all/night2day_58_5018_to_5000_real_A}{}{fig:bag_2}
    \addSubFigEighth{our_baseline_all/night2day_58_5018_to_5000_fake_B}{}{fig:bag_3}
    \caption{Our Architecture (conditioning all layers) Multi-Task}
    \label{fig:channel_gated_multi-task}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}%[ht!]
    \centering
    \addSubFigEighth{acgan_baseline_all/cityscapes_95_real_A}{}{fig:bag_sketch} 
    \addSubFigEighth{acgan_baseline_all/cityscapes_95_fake_B}{}{fig:bag_1} 
    \addSubFigEighth{acgan_baseline_all/night2day_58_5018_to_5000_real_A}{}{fig:bag_2}
    \addSubFigEighth{acgan_baseline_all/night2day_58_5018_to_5000_fake_B}{}{fig:bag_3}
    \caption{Our Architecture (conditioning all layers) Multi-Task}
    \label{fig:channel_gated_multi-task}
    \vspace{-3mm}
\end{figure*}

\section{Planned Set of Experiments:}
\subsection{Unconditional Generations :}
Since the model is not restricted to be applicable only in the image to image setting and is more general than that, if we get the unconditional generations working at least on the places dataset and the faces(it also has got some class labels that can be used for conditioned generation). It will be a good generalization. A bit of engineering might be required to get the architecture working on the unconditional case since the structure of the generator and discriminator would be different from the current generator which takes an image as an input and outputs an image of the same resolution. The discriminator in the case of the current image2image experiments employ a patch based discriminator which has to be modified to work in the unconditional generation setting.




% \subsection{Instance Based Generations: }
% As demonstrated by the early experiments I performed with pix2pixhd that it overfits the dataset and simplification of the same instance map led to garbled generations showed that it was unstable to perturbations. Instance based generations could be possible with our model since we already know which pixels are for which class and we can generate instances of each class separately and then stitch together the various class generations into a final image.


% \section{Directions about novelty of approach:} 

% \subsection{Learning to Learn}
% Learning to learn is becoming an increasingly relevant paradigm for deep learning models as the power of the networks increase and we want less hyper parameter tuning. Our model can be perceived as a system whereby the hypernetwork responsible for predicting the $\alpha_i$ of each block is learning by analyzing the function learnt by each block and thereby distributing the blocks between the various different classes for the generator and the discriminator. We will have to look deeply into the literature to identify the connections with the learning to learn paradigm.

% \subsection{Ensembles of several shallower nets (implicit MAD-GAN)}
% The initial motivation of Eli and Oliver was to extend MAD-GAN with the intuitions gained by Andreas Veit's paper on Residual Blocks acting as ensembles of several shallower networks. The structure of the paper at the moment follows that intuition and the experiments on the 1D Mixture of Gaussians demonstrates that residual blocks indeed behave as ensembles of several shallower networks even in the case of generative models.

% \subsection{Effective way of fusing high dimensional and low dimensional information}
% The techniques proposed in this paper provides a way of effectively fusing high dimensional information in the form of images and low dimensional conditioning in terms of class conditioning or sampled conditioning as in the form of InfoGAN. Further the multi-task application demonstrates the efficacy of the fusing of task information alongside the high dimensional information about images.

