\section{Preliminaries}

\subsection{\ow{these are from intro}}

\ow{moved, we don't want to make us just look like veit++}
There was an interesting set of experiments performed in \cite{veit2016residual} which showed that residual networks behaved as an ensemble of several shallower networks.
In one interesting experiment  some of the residual blocks were removed out of a trained network and the incised network was still able to perform almost as well inspite of several of these residual blocks being bypassed. 
The above principles led us to a model based on a soft gating mechanism whereby a hypernetwork gets the condition modulates the feature activations of the residual blocks i.e. the output of the standard residual block was modified from $x+f(x)$ to  $x+\alpha . f(x)$ where the set of alphas for each of the residual block is predicted by the hypernetwork. 


Our experiments with the gate selection block on the quintessential MNIST dataset show its efficacy in a limited setting, we further show that infogan based network could be applied to the pix2pix experiments which was earlier known to produce the delta distribution and only a single plausible output \cite{ghosh2017multi}. The resulting infogan based network was able to produce stochastic variations on the generated samples for the task of edges to handbags. We further show the efficacy of the model in the case when we have explicit class information and use it for a novel task of multi-class scribble to image generation. 

We show an interesting set of experiments in the 1D setting of Mixture of Gaussians as was performed in \cite{ghosh2017multi}. A model was trained with the generator composed of residual blocks and then in line with the experiments performed in \cite{veit2016residual} we removed certain residual blocks and allowed the information to flow through the skip connection and we found that removal of certain modes corresponded to the removal of certain residual blocks hence supporting the arguments in \cite{veit2016residual} that residual networks behave as an ensemble of several shallower networks even in the case of generative models.





\subsection*{GANs:} 


\begin{align}\label{eq:ganObjective}
     \min_{\theta_g}\max_{\theta_d}\; & V(\theta_d, \theta_g) := \E_{x \sim p_{d}} \log D(x; \theta_d) \nonumber \\&
     + \E_{z \sim p_z } \log \big( 1 - D(G(z; \theta_g) ; \theta_d)\big) 
\end{align}



Here we present a brief review of GANs~\cite{goodfellow2014generative}. Given a set of samples $\mathcal{D} = (x_i)_{i=1}^n$ from the true data distribution $p_d$, the GAN learning problem is to obtain the optimal parameters $\theta_g$ of a generator $G(z;\theta_g)$ that can sample from an approximate data distribution $p_g$, where $z \sim p_z$ is the prior input noise (\eg samples from a normal distribution). In order to learn the optimal $\theta_g$, the GAN objective (Eq. \eqref{eq:ganObjective}) employs a discriminator $D(x; \theta_d)$ that learns to differentiate between a {\em real} (from $p_d$) and a {\em fake} (from $p_g$) sample $x$. The overall GAN objective is:


The above objective is optimized in a block-wise manner where $\theta_d$ and $\theta_g$ are optimized one at a time while fixing the other. For a given sample $x$ (either from $p_d$ or $p_g$) and the parameter $\theta_d$, the function $D(x; \theta_d) \in [0, 1]$ produces a score that represents the probability of $x$ belonging to the true data distribution $p_d$ (or probability of it being real). The objective of the discriminator is to learn parameters $\theta_d$ that maximizes this score for the true samples (from $p_d$) while minimizing it for the fake ones $\tilde{x} = D(z; \theta_g)$ (from $p_g$). In the case of generator, the objective is to minimize $\E_{z \sim p_z}\log \big( 1 - D(G(z; \theta_g) ; \theta_d) \big)$, equivalently maximize $\E_{z \sim p_z} \log D(G(z; \theta_g) ; \theta_d)$. Thus, the generator learns to maximize the scores for the fake samples (from $p_g$), which is exactly the opposite to what discriminator is trying to achieve. In this manner, the generator and the discriminator are involved in a minimax game where the task of the generator is to maximize the mistakes of the discriminator. Theoretically, at equilibrium, the generator learns to generate real samples, which means $p_g = p_d$.

\subsection*{Residual Networks:}
There was the standard degradation problem in deep neural networks that even after adding more layers to a network the performance of the network doesn't increase . Residual Networks\cite{he2016deep} address the degradation problem by introducing a deep residual learning framework. Instead of hoping each few stacked layers directly fit a desired underlying mapping, they explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as H(x), they let the stacked nonlinear layers fit another mapping of $F(x):=H(x)-x$. The original mapping is recast into $F(x)+x$. The hypothesis is that it is easier to optimize the residual mapping than to optimize the original mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.



\subsection*{Generative Network Incision:}
We began with some experiments on a 1D scenario of 1D Mixture of Gaussians, since GANs are generative models, it is apt that we first understand the effect of residual blocks on the Generator of a GAN first in a simple scenario. We first trained a generator and discriminator pair built entirely of residual blocks on the simple setting of 1D distribution. Subsequently on a similar vein as in \cite{veit2016residual} some incision experiments were performed to analyze the performance of the various residual blocks in this setting. An interesting observation was that when some residual blocks were disabled and only the skip connection of that block was activated it led to the disappearance of particular modes from the data distribution. Removal of groups of blocks led to the disappearance of clusters of modes from the generated distribution. This experiment laid the foundation for our Gated Residual Blocks.

\begin{figure*}%[ht!]
    \centering
    \addSubFigHalf{Picture13.png}{Gated Residual Blocks for the Generator}{fig:gru_gen} 
    \addSubFigHalf{Picture14.png}{Gated Residual Blocks for the Discriminator}{fig:gru_dis} 
    \caption{Gated Residual Blocks in Generator and Discriminator}
    \label{fig:model}
    \vspace{-3mm}
\end{figure*}

\section{Gated Residual Block based Generator}
Inspired by the incision experiments performed on the generator whereby removal of certain blocks led to the removal of particular modes from the data distribution, our model consists of a main network which is oblivious to the condition provided to the network, while another hypernetwork only receives the condition and has to predict which block should be used to what extent. More precisely, the $i^{th}$ residual block now receives an extra input $\alpha_i$ alongside the usual $x$ and the output of the gated residual block is $x+\alpha_i*f_i(x)$ rather than the standard $x+f_i(x)$. The $alpha_i$s are predicted via another hypernetwork which only receives the condition and has no idea about the input being received by the main block. The interpretation of the above is that if some block doesn't have to be used for a particular class then the hypernetwork can just choose the $alpha_i$ close to 0 and effectively that block is switched off. The intuition is that the hypernetwork has to first understand the transformations that the different residual blocks in the generator are learning, then start modulating it such that conditioned on the class the required blocks are chosen to the right extent such that the resulting sequence of transformations corresponds to realistic images from that particular class. Its related to FILM \cite{perez2017film} albeit it does feature wise transform and has to predict more parameters than a single number per block. \figref{fig:gru_gen} illustrates the concept in the setting of the conditional generator. Some other extensions such as affine gating per residual block and channel wise gating with its affine counterpart exists as well apart from the well known Adaptive Instance Normalization \cite{huang2017arbitrary} . The varied forms of gating could be applied to the Infogan setup with the gate prediction block receives the randomly sampled latent as input to decide the gates on the various blocks while the Q network trying to reconstruct back the latent that was passed.
 
\section{Gated Residual Block based Discriminator}

Based on a similar principle as the Generator, the Discriminator can also be equipped with gated residual blocks whereby each residual block would compute $x+\alpha_i*f_i(x)$ in place of the standard $x+f_i(x)$ where each $alpha_i$ is predicted by another hypernetwork which gets the condition that which class is the network currently judging for real/fake. Its intriguing that with just the class information the hypernetwork is able to select blocks which effectively guide the discriminator to judge whether its real/fake conditioned on the class. Via accurate gradients backpropagated to the generator it also enables the generator to generate class conditioned high resolution image samples. Intuitively, the discriminator distributes some common functions between the different classes to some of these shared residual blocks which are activated for all classes while the rest of the transformations it distributes in a non-overlapping manner to some specific residual blocks of the discriminator network. Such a concept can not only be used for the discriminator but in many settings where the conditioning variable is known, for example some plausible applications can be the Q network in InfoGAN\cite{chen2016infogan} or the Conditional Inference Network in a CVAE \cite{sohn2015learning}. \figref{fig:gru_dis} illustrates the concept in the setting of the conditional discriminator. Some other extensions such as affine gating per residual block and channel wise gating with its affine counterpart exists as well apart from Adaptive Instance Normalization(AdaIN) \cite{huang2017arbitrary}
